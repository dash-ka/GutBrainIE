{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI\n",
    "import re, os, json, pysbd\n",
    "import prompts\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API\"\n",
    "client = OpenAI()\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\"\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_spans(sentences, entities, location):\n",
    "\n",
    "          abstract_data = list()\n",
    "        \n",
    "          current_position = 0\n",
    "          sentence_spans = []\n",
    "          for sentence in sentences:\n",
    "               sentence_length = len(sentence)\n",
    "                 \n",
    "               sentence_spans.append((current_position, current_position + sentence_length))\n",
    "               current_position += sentence_length  + 1\n",
    "        \n",
    "          for i, (sentence_start, sentence_end) in enumerate(sentence_spans):\n",
    "               sentence_text = sentences[i]\n",
    "            \n",
    "               annotations = []\n",
    "               for entity in entities:\n",
    "\n",
    "                         entity_type = entity.get(\"label\")\n",
    "                         entity_start = int(entity[\"start_idx\"])\n",
    "                         entity_end = int(entity[\"end_idx\"]) +1\n",
    "                         entity_mention = entity[\"text_span\"]\n",
    "               \n",
    "\n",
    "                         if sentence_start <= entity_start < sentence_end:\n",
    "                              new_start = entity_start - sentence_start\n",
    "                              new_end = entity_end - sentence_start\n",
    "                              if new_end >  sentence_end-sentence_start:\n",
    "                                   esent = sentence_text + \" \"+sentences[i+1]\n",
    "                              else:\n",
    "                                   esent = sentence_text\n",
    "                         \n",
    "                              if esent[new_start:new_end]!=entity_mention:\n",
    "                              \n",
    "                                   print(esent)\n",
    "                                   print(entity_start, entity_end)\n",
    "                                   print(new_start, new_end, current_position)\n",
    "                                   print(sentence_start, sentence_end)\n",
    "                                   print(esent[new_start:new_end], entity_mention)\n",
    "                                 \n",
    "                              annotations.append({\n",
    "                         \"label\": entity_type,\n",
    "                         \"start\": entity_start, \n",
    "                         \"end\": entity_end, \n",
    "                         \"new_start\":new_start,\n",
    "                         \"new_end\": new_end,\n",
    "                         \"mention\": entity_mention,\n",
    "                         \"location\": entity[\"location\"]\n",
    "                              })\n",
    "               abstract_data.append(\n",
    "                 \n",
    "                 {    \n",
    "                      \"location\":location,\n",
    "                      \"start_sent\":sentence_start,\n",
    "                      \"text\":sentence_text,\n",
    "                      \"annotations\": annotations\n",
    "                 })\n",
    "          return  abstract_data\n",
    "\n",
    "def prompt_llm(system, sentence, model=None):\n",
    "    client = OpenAI\n",
    "    user = \"\\nText:\\n```\\n\" + sentence + \"\\n```\\n\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": system},\n",
    "          {\"role\": \"user\", \"content\": user}\n",
    "        ],\n",
    "    )\n",
    "    try:\n",
    "        return response.choices[0].message.content\n",
    "    except TypeError:\n",
    "        print(response)\n",
    "        raise TypeError\n",
    "\n",
    "def tag_entities(text, entities):\n",
    "    entities_sorted = sorted(entities, key=lambda e: e['start'], reverse=True)\n",
    "    \n",
    "    for entity in entities_sorted:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        entity_type = entity['label']\n",
    "        if entity_type in [\"DDF\", \"ddf\"]:\n",
    "            entity_type = \"disease or finding\"\n",
    "        \n",
    "        text = text[:end] + f\"</entity>\" + text[end:]\n",
    "        text = text[:start] + f\"<entity type=\\\"{entity_type}\\\">\" + text[start:]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_and_parse_json(text):\n",
    "    match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            json_str = re.sub(\";}\",\"}\", json_str.strip())\n",
    "            json_str = re.sub(\",}\",\"}\", json_str.strip())\n",
    "            json_dict = json.loads(json_str)\n",
    "            print(json_dict)\n",
    "            if isinstance(json_dict.get(\"Relational triples\"), str):\n",
    "                json_dict = {\"Relational triples\":[json_dict.get(\"Relational triples\")]}\n",
    "            return json_dict\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "          print(\"Error in decoding json string: \", json_str)\n",
    "          return {\"Relational triples\": []}\n",
    "    else:\n",
    "        return {\"Relational triples\": []}\n",
    "    \n",
    "def extract_spo(triple_str):\n",
    "    \"\"\"\n",
    "    Extracts subject, predicate, and object from a string formatted as \"[subject] PREDICATE [object]\".\n",
    "    \n",
    "    Args:\n",
    "        triple_str (str): The input string.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (subject, predicate, object)\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[(.*?)\\]\\s+(.+?)\\s+\\[(.*?)\\]\"\n",
    "    match = re.match(pattern, triple_str)\n",
    "    \n",
    "    if match:\n",
    "        subject, predicate, obj = match.groups()\n",
    "        return subject.strip(), predicate.strip(), obj.strip()\n",
    "    else:\n",
    "        return \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load GliNER predictions and sentence-segment test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split abstracts into sentences\n",
    "\n",
    "file =\"data/predictions_GLINER.json\"\n",
    "with open(file) as fin:\n",
    "    data = json.load(fin)\n",
    "    for pid in data:\n",
    "        title = data[pid][\"metadata\"][\"title\"]\n",
    "        abstract = data[pid][\"metadata\"][\"abstract\"]\n",
    "        entities = data[pid][\"entities\"]\n",
    "        sent_data = []\n",
    "        for (t, loc) in [(title, \"title\"), (abstract, \"abstract\")]:\n",
    "            sentences = re.split(r\"(?<=\\.) (?=[A-Z])\", t)\n",
    "            new_sents = []\n",
    "            for s in sentences: \n",
    "                if len(s) > 400:\n",
    "                    new_sents.extend(seg.segment(s))\n",
    "                else:\n",
    "                    new_sents.append(s)\n",
    "                ents = [e|{\"location\":loc} for e in entities if e[\"location\"]==loc]\n",
    "                sent_data.extend(recalculate_spans(sentences, ents, loc))\n",
    "            data[pid] = {\"sent_data\":sent_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tag sentences with extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in data:\n",
    "    for sid, s in enumerate(data[pid][\"sent_data\"]):\n",
    "        text = s[\"text\"]\n",
    "        entities = s[\"annotations\"]\n",
    "        all_entities = []\n",
    "\n",
    "        for e in entities:\n",
    "            if text[e[\"new_start\"]:e[\"new_end\"]] == e[\"mention\"]:\n",
    "                all_entities.append({\n",
    "                        \"start\":e[\"new_start\"], \n",
    "                        \"end\": e[\"new_end\"],\n",
    "                        \"location\":e[\"location\"], \n",
    "                        \"mention\":e[\"mention\"],\n",
    "                        \"label\": e[\"label\"]\n",
    "                        })\n",
    "            else:\n",
    "                print(\"Wrong spans: \", text[e[\"new_start\"]:e[\"new_end\"]], e[\"mention\"])\n",
    "        data[pid][\"sent_data\"][sid][\"tagged_text\"] = tag_entities(text, all_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prompt OpenAI model to extract specified relation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_types = [\"LOCATED_IN\", \"INTERACT\", \"IMPACT\", \"LINKED_TO\", \"AFFECT\", \"USED_BY\"]\n",
    "\n",
    "for sem_type in relation_types:\n",
    "\n",
    "    prompt_name = getattr(prompts, sem_type)\n",
    "\n",
    "    for pid in tqdm(data):\n",
    "\n",
    "        for sid, sent in enumerate(data[pid][\"sent_data\"]):\n",
    "\n",
    "            entities = sent[\"annotations\"]\n",
    "            sentence = sent[\"tagged_text\"]\n",
    "            \n",
    "            if \"pred_relations\" not in data[pid][\"sent_data\"][sid]:\n",
    "                data[pid][\"sent_data\"][sid][\"pred_relations\"] = []\n",
    "\n",
    "            if entities and entities is not None:\n",
    "                            \n",
    "                response = prompt_llm(prompt_name, sentence, model=MODEL_NAME)\n",
    "                data[pid][\"sent_data\"][sid][\"pred_relations\"].append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Filter extracted relations based on legal relational patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_relations = [\n",
    " (\"anatomical location\", \"human\", \"located in\"),\n",
    " (\"anatomical location\", \"animal\", \"located in\"),\n",
    " (\"bacteria\", \"bacteria\", \"interact\"),\n",
    " (\"bacteria\", \"chemical\", \"interact\"),\n",
    " (\"bacteria\", \"ddf\", \"influence\"),\n",
    " (\"bacteria\", \"gene\", \"change expression\"),\n",
    " (\"bacteria\", \"human\", \"located in\"),\n",
    " (\"bacteria\", \"animal\", \"located in\"),\n",
    " (\"bacteria\", \"microbiome\", \"part of\"),\n",
    " (\"chemical\", \"animal location\", \"located in\"),\n",
    " (\"chemical\", \"human\", \"located in\"),\n",
    " (\"chemical\", \"animal\", \"located in\"),\n",
    " (\"chemical\", \"chemical\", \"interact\"),\n",
    " (\"chemical\", \"chemical\", \"part of\"),\n",
    " (\"chemical\", \"microbiome\", \"impact\"),\n",
    " (\"chemical\", \"microbiome\", \"produced by\"),\n",
    " (\"chemical\", \"bacteria\", \"impact\"),\n",
    " (\"dietary supplement\", \"bacteria\", \"impact\"),\n",
    " (\"drug\", \"bacteria\", \"impact\"),\n",
    " (\"food\", \"bacteria\", \"impact\"),\n",
    " (\"chemical\", \"microbiome\", \"impact\"),\n",
    " (\"dietary supplement\", \"microbiome\", \"impact\"),\n",
    " (\"drug\", \"microbiome\", \"impact\"),\n",
    " (\"food\", \"microbiome\", \"impact\"),\n",
    " (\"chemical\", \"ddf\", \"influence\"),\n",
    " (\"dietary supplement\", \"ddf\", \"influence\"),\n",
    " (\"food\", \"ddf\", \"influence\"),\n",
    " (\"chemical\", \"gene\", \"change expression\"),\n",
    " (\"dietary supplement\", \"gene\", \"change expression\"),\n",
    " (\"drug\", \"gene\", \"change expression\"),\n",
    " (\"food\", \"gene\", \"change expression\"),\n",
    " (\"chemical\", \"human\", \"administered\"),\n",
    " (\"dietary supplement\", \"human\", \"administered\"),\n",
    " (\"drug\", \"human\", \"administered\"),\n",
    " (\"food\", \"human\", \"administered\"),\n",
    " (\"chemical\", \"animal\", \"administered\"),\n",
    " (\"dietary supplement\", \"animal\", \"administered\"),\n",
    " (\"drug\", \"animal\", \"administered\"),\n",
    " (\"food\", \"animal\", \"administered\"),\n",
    " (\"ddf\", \"anatomical location\", \"strike\"),\n",
    " (\"ddf\", \"bacteria\", \"change abundance\"),\n",
    " (\"ddf\", \"microbiome\", \"change abundance\"),\n",
    " (\"ddf\", \"chemical\", \"interact\"),\n",
    " (\"ddf\", \"ddf\", \"affect\"),\n",
    " (\"ddf\", \"ddf\", \"is a\"),\n",
    " (\"ddf\", \"human\", \"target\"),\n",
    " (\"ddf\", \"animal\", \"target\"),\n",
    " (\"drug\", \"chemical\", \"interact\"),\n",
    " (\"drug\", \"drug\", \"interact\"),\n",
    " (\"drug\", \"ddf\", \"change effect\"),\n",
    " (\"human\", \"biomedical technique\", \"used by\"),\n",
    " (\"animal\", \"biomedical technique\", \"used by\"),\n",
    " (\"microbiome\", \"biomedical technique\", \"used by\"),\n",
    " (\"microbiome\", \"anatomical location\", \"located in\"),\n",
    " (\"microbiome\", \"human\", \"located in\"),\n",
    " (\"microbiome\", \"animal\", \"located in\"),\n",
    " (\"microbiome\", \"gene\", \"change expression\"),\n",
    " (\"microbiome\", \"ddf\", \"is linked to\"),\n",
    " (\"microbiome\", \"microbiome\", \"compared to\")\n",
    " ]\n",
    "\n",
    "unspecified_entities = [\n",
    "    \"bacteria\", \n",
    "    \"metabolites\",\n",
    "    \"metabolite\",\n",
    "    \"genes\", \n",
    "    \"gene\",\n",
    "    \"disease\", \n",
    "    \"diseases\", \n",
    "    \"disorder\",\n",
    "    \"disorders\",\n",
    "    \"microbiome\", \n",
    "    \"microbiota\",\n",
    "    \"human\", \n",
    "    \"animal\", \n",
    "    \"food\", \n",
    "    \"chemical\", \n",
    "    \"chemicals\", \n",
    "    \"drug\", \"drugs\",\n",
    "    \"neurotransmitters\", \n",
    "    \"neurotransmitter\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in tqdm(list(data)):\n",
    "    \n",
    "    relations = []\n",
    "    for sid, sent in enumerate(data[pid][\"sent_data\"]):\n",
    "        entities = sent[\"annotations\"]\n",
    "        sentence = sent[\"tagged_text\"]\n",
    "        sent_spos = []\n",
    "        \n",
    "        for raw_rel in sent.get(\"pred_relations\", []):\n",
    "            if raw_rel.strip():\n",
    "                \n",
    "                triples = extract_and_parse_json(raw_rel.strip())\n",
    "                triples = triples.get(\"Relational triples\", [])\n",
    "                ss, so = [], []\n",
    "                for triple in triples:\n",
    "                    subj, predicate, obj = extract_spo(triple)\n",
    "\n",
    "                    for e in list(entities):\n",
    "                        if (e[\"mention\"].lower() == subj.lower()) and \\\n",
    "                            (subj.lower() not in unspecified_entities):\n",
    "                            ss.append(e)\n",
    "                            break\n",
    "                    \n",
    "                    for e in entities:\n",
    "                        if (e[\"mention\"].lower() == obj.lower()) and \\\n",
    "                            (obj.lower() not in unspecified_entities):\n",
    "                            so.append(e)\n",
    "                            break\n",
    "                    \n",
    "                    spos = []\n",
    "                    if ss and so:\n",
    "                        for s in ss:\n",
    "                            for o in so:\n",
    "                                spos.append([s, o, predicate])\n",
    "\n",
    "                    for spo in spos:\n",
    "                        lab1, lab2 = spo[0][\"label\"].lower(), spo[1][\"label\"].lower()\n",
    "                        rel_type = spo[-1].lower()\n",
    "\n",
    "                        # housekeeping\n",
    "                        if rel_type == \"associated with\":\n",
    "                            rel_type = \"affect\"\n",
    "\n",
    "                        lab1 = \"DDF\" if lab1.lower() in [\"ddf\", \"disease or finding\"] else lab1\n",
    "                        lab2 = \"DDF\" if lab2.lower() in [\"ddf\", \"disease or finding\"] else lab2\n",
    "                        \n",
    "                        if (lab1.lower(), lab2.lower(), rel_type.lower()) in legal_relations:\n",
    "                            key = (spo[0][\"mention\"],lab1,rel_type,spo[1][\"mention\"],lab2)\n",
    "                            if key not in sent_spos:\n",
    "                                sent_spos.append(key)\n",
    "\n",
    "        relations.extend(sent_spos)\n",
    "\n",
    "    ternary_mention_based_relations = []\n",
    "    ternary_tag_based_relations = []\n",
    "    binary_tag_based_relations = []\n",
    "\n",
    "    for item in relations:\n",
    "        ternary_mention_based_relations.append(\n",
    "            {\n",
    "                'subject_text_span': item[0],\n",
    "                'subject_label': item[1],\n",
    "                'predicate':item[2],\n",
    "                'object_text_span':item[3],\n",
    "                'object_label':  item[4]\n",
    "            }\n",
    "        )\n",
    "        ternary_tag_based_relations.append(\n",
    "            {\n",
    "                'subject_label': item[1],\n",
    "                'predicate':item[2],\n",
    "                'object_label':  item[4] \n",
    "            }\n",
    "        )\n",
    "        binary_tag_based_relations.append(\n",
    "            {\n",
    "                'subject_label': item[1],\n",
    "                'object_label':  item[4]\n",
    "            }\n",
    "        )\n",
    "    data[pid][\"ternary_mention_based_relations\"] = ternary_mention_based_relations\n",
    "    data[pid][\"ternary_tag_based_relations\"] = ternary_tag_based_relations\n",
    "    data[pid][\"binary_tag_based_relations\"] = binary_tag_based_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_re = dict()\n",
    "binary_re = dict()\n",
    "mention_re = dict()\n",
    "\n",
    "for pid in data:\n",
    "    tag_re[pid] = {\"ternary_tag_based_relations\":data[pid][\"ternary_tag_based_relations\"]}\n",
    "    binary_re[pid] = {\"binary_tag_based_relations\":data[pid][\"binary_tag_based_relations\"]}\n",
    "    mention_re[pid] = {\"ternary_mention_based_relations\":data[pid][\"ternary_mention_based_relations\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/Unsupervised_T622_R1_gpt4re/Unsepervised_T622_R1_gpt4re.json\", \"w\") as file:\n",
    "    json.dump(binary_re, file, indent=4)\n",
    "\n",
    "with open(\"outputs/Unsupervised_T623_R1_gpt4re/Unsepervised_T623_R1_gpt4re.json\", \"w\") as file:\n",
    "    json.dump(tag_re, file, indent=4)\n",
    "\n",
    "with open(\"outputs/Unsupervised_T624_R1_gpt4re/Unsepervised_T624_R1_gpt4re.json\", \"w\") as file:\n",
    "    json.dump(mention_re, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/llm_results.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
